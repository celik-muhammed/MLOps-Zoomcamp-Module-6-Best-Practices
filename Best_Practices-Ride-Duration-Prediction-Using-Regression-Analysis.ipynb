{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2525fb2f",
   "metadata": {},
   "source": [
    "<div style=\"align: center; margin: 0; padding: 0; height: 250px;\">\n",
    "    <br>\n",
    "    <img src=\"https://www.nyc.gov/assets/tlc/images/content/hero/MRP-Closing-Week.jpg\" style=\"display:block; margin:auto; width:65%; height:100%;\">\n",
    "</div><br><br> \n",
    "\n",
    "<div style=\"letter-spacing:normal; opacity:1.;\">\n",
    "<!--   https://xkcd.com/color/rgb/   -->\n",
    "  <p style=\"text-align:center; background-color: lightsalmon; color: Jaguar; border-radius:10px; font-family:monospace; \n",
    "            line-height:1.4; font-size:32px; font-weight:bold; text-transform: uppercase; padding: 9px;\">\n",
    "            <strong>TLC Trip Record Data</strong></p>  \n",
    "  \n",
    "  <p style=\"text-align:center; background-color:romance; color: Jaguar; border-radius:10px; font-family:monospace; \n",
    "            line-height:1.4; font-size:22px; font-weight:normal; text-transform: capitalize; padding: 5px;\"\n",
    "     >Machine Learning Module: MONITORING - Ride Duration Prediction<br>using Regression Analysis<br></p><br>\n",
    "    \n",
    "  <div style=\"align: center;\">\n",
    "  <table style=\"text-align: center; background-color: romance; color: Jaguar; border-radius: 10px; font-family: monospace;\n",
    "                  line-height:1.4; font-size: 21px; font-weight: normal; text-transform: capitalize; padding: 5px; \n",
    "                  margin: 0 auto;\">\n",
    "    <tr><td style=\"text-align: left; padding-left: 0px;\"\n",
    "            > MONITORING <span style=\"font-size: 16px;\">(to ensure that deployed models remain accurate,<br>reliable, and aligned with business objectives over time)</span></td></tr>\n",
    "    <tr><td style=\"text-align: left; padding-left: 0px;\"\n",
    "            > MLOps <span style=\"font-size: 16px;\">(CI/CD, Model Versioning, Monitoring, Automated Retraining,<br>Security, Scalability, Collaboration)</span></td></tr>\n",
    "  </table>\n",
    "  </div>\n",
    "\n",
    "</div>\n",
    "\n",
    "[Introduction to MLOps Monitoring](https://github.com/dimzachar/mlops-zoomcamp/blob/master/notes/Week_5/intro.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a8b00d",
   "metadata": {},
   "source": [
    "**Dataset Info**\n",
    "\n",
    "\n",
    "**Context**\n",
    "\n",
    "Yellow and green taxi trip records include fields capturing pick-up and drop-off dates/times, pick-up and drop-off locations, trip distances, itemized fares, rate types, payment types, and driver-reported passenger counts. The data used in the attached datasets were collected and provided to the NYC Taxi and Limousine Commission (TLC) by technology providers authorized under the Taxicab & Livery Passenger Enhancement Programs (TPEP/LPEP). The trip data was not created by the TLC, and TLC makes no representations as to the accuracy of these data.\n",
    "\n",
    "For-Hire Vehicle (“FHV”) trip records include fields capturing the dispatching base license number and the pick-up date, time, and taxi zone location ID (shape file below). These records are generated from the FHV Trip Record submissions made by bases. Note: The TLC publishes base trip record data as submitted by the bases, and we cannot guarantee or confirm their accuracy or completeness. Therefore, this may not represent the total amount of trips dispatched by all TLC-licensed bases. The TLC performs routine reviews of the records and takes enforcement actions when necessary to ensure, to the extent possible, complete and accurate information.\n",
    "\n",
    "\n",
    "**ATTENTION!**\n",
    "\n",
    "On 05/13/2022, we are making the following changes to trip record files:\n",
    "\n",
    "- All files will be stored in the PARQUET format. Please see the ‘Working With PARQUET Format’ under the Data Dictionaries and MetaData section.\n",
    "- Trip data will be published monthly (with two months delay) instead of bi-annually.\n",
    "- HVFHV files will now include 17 more columns (please see High Volume FHV Trips Dictionary for details). Additional columns will be added to the old files as well. The earliest date to include additional columns: February 2019.\n",
    "- Yellow trip data will now include 1 additional column (‘airport_fee’, please see Yellow Trips Dictionary for details). The additional column will be added to the old files as well. The earliest date to include the additional column: January 2011.\n",
    "\n",
    "\n",
    "**Download the data for March 2023**\n",
    "\n",
    "Dataset: https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
    "\n",
    "\n",
    "**Data Dictionaries and MetaData**\n",
    "\n",
    "- We'll use the same `NYC taxi dataset`, but instead of \"Yellow Taxi Trip Records\", we'll use `\"Green Taxi Trip Records\"`.\n",
    "\n",
    "> `Green Trips Data Dictionary`: https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_green.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b74ab3",
   "metadata": {},
   "source": [
    "**TASK**\n",
    "\n",
    "\n",
    "In this homework, we'll take the ride duration prediction model that we deployed in batch mode in homework 4 and improve the reliability of our code with unit and integration tests. \n",
    "\n",
    "[homework](https://github.com/DataTalksClub/mlops-zoomcamp/blob/main/cohorts/2023/06-best-practices/homework.md)\n",
    "\n",
    "\n",
    "\n",
    "**Table of Content**\n",
    "\n",
    "\n",
    "1. Import Libraries and Ingest Data \n",
    "2. Docker \n",
    "3. Baseline Model nyc_taxi Data\n",
    "    - Q1. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78138c11",
   "metadata": {},
   "source": [
    "<div style=\"letter-spacing:normal; opacity:1.;\">\n",
    "  <h1 style=\"text-align:center; background-color: lightsalmon; color: Jaguar; border-radius:10px; font-family:monospace; border-radius:20px;\n",
    "            line-height:1.4; font-size:32px; font-weight:bold; text-transform: uppercase; padding: 9px;\">\n",
    "            <strong>1. Import Libraries & Ingest Data</strong></h1>   \n",
    "</div>\n",
    "\n",
    "> ⚠️ Not Recommended conda `base` env, work on `venv`\n",
    "\n",
    "- https://docs.conda.io/projects/conda/en/4.6.0/_downloads/52a95608c49671267e40c689e0bc00ca/conda-cheatsheet.pdf\n",
    "\n",
    "```\n",
    "pip freeze > requirements.txt\n",
    "conda list -e > requirements.txt\n",
    "\n",
    "# new conda virtual environment\n",
    "conda create --name \"pytest-ops\" python=3.10 jupyter -y\n",
    "conda activate \"pytest-ops\"\n",
    "\n",
    "# install all package dependencies\n",
    "pip install -r requirements.txt\n",
    "conda install -c conda-forge --file=requirements.txt      # mostly not work\n",
    "conda install -c conda-forge pandas==2.0.2 -q -y\n",
    "\n",
    "# if The environment is inconsistent, try below\n",
    "conda update -n base -c defaults conda --force-reinstall\n",
    "conda install anaconda --force-reinstall\n",
    "\n",
    "```\n",
    "- pipenv\n",
    "```sh\n",
    "pip install pipenv\n",
    "pipenv shell        # or pipenv run\n",
    "pipenv --where\n",
    "pipenv --venv\n",
    "```\n",
    "\n",
    "**You must use the `--no-deps` option in the pip install command in order to avoid bundling dependencies into your conda-package.**\n",
    "\n",
    "If you run pip install without the `--no-deps` option, pip will often install dependencies in your conda recipe and those dependencies will become part of your package. This wastes space in the package and `increases the risk of file overlap`, file clobbering, and broken packages.\n",
    "\n",
    "There might be cases where you want to install a package directly from a local directory or a specific location, without relying on the package indexes. In such situations, you can use the `--no-index` option to tell pip not to look for the package in any indexes.\n",
    "\n",
    "```\n",
    "- command1 & command2  # runs simultaneously\n",
    "- command1 ; command2  # runs sequentially\n",
    "- command1 && command2 # runs sequentially, runs command2 only if command1 succeeds\n",
    "- command1 || command2 # runs sequentially, runs command2 only if command1 fails\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74723240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt \n",
    "# To get started with MLflow you'll need to install the appropriate Python package.\n",
    "\n",
    "# for parquet file\n",
    "pyarrow==11.0.0\n",
    "fastparquet==2023.4.0\n",
    "\n",
    "pandas==2.0.2\n",
    "matplotlib==3.7.1\n",
    "orjson==3.8.8          # orjson is a fast, correct JSON library\n",
    "tqdm==4.65.0\n",
    "# requests==2.29.0\n",
    "\n",
    "# ML Model packages\n",
    "scikit-learn==1.2.2\n",
    "\n",
    "# MLOPS packages\n",
    "mlflow==2.3.1\n",
    "prefect==2.10.18\n",
    "prefect-email==0.2.2\n",
    "\n",
    "# MLOPS Cloud packages\n",
    "boto3~=1.24.28\n",
    "prefect-aws==0.3.4\n",
    "\n",
    "# Monitoring\n",
    "joblib==1.2.0\n",
    "evidently==0.3.3\n",
    "psycopg==3.1.9\n",
    "# psycopg[binary]==3.1.9\n",
    "\n",
    "# Optionally\n",
    "pipenv\n",
    "jupyter\n",
    "ipykernel\n",
    "ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7db08661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n",
      "Python  : 3.10.11 (main, Apr 20 2023, 19:02:41) [GCC 11.2.0]\n",
      "Platform: Linux Linux-5.15.90.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\n",
      "Actv Env: base\n"
     ]
    }
   ],
   "source": [
    "import os, sys, platform, IPython.display\n",
    "\n",
    "# !{sys.executable} -m pip install -Uq -r requirements.txt  #  --no-deps --no-cache-dir --force-reinstall --no-index\n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "\n",
    "# IPython.display.clear_output()\n",
    "print(\"Python  :\", sys.version)\n",
    "print(\"Platform:\", platform.system(), platform.platform())\n",
    "print(\"Actv Env:\", os.environ['CONDA_DEFAULT_ENV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81ffeeb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "from numba import NumbaDeprecationWarning\n",
    "# Suppress NumbaDeprecationWarning in umap.distances module\n",
    "warnings.filterwarnings(\"ignore\", category=NumbaDeprecationWarning)\n",
    "\n",
    "import os\n",
    "# TensorFlow to only display error messages \n",
    "# and suppress warnings and informational messages.\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy\n",
    "from scipy.stats import stats\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import click\n",
    "import pickle\n",
    "import datetime\n",
    "# import pathlib\n",
    "# import argparse\n",
    "import requests\n",
    "# import urllib.request\n",
    "from glob import glob\n",
    "\n",
    "# Import joblib for model persistence and tqdm for progress bars\n",
    "from joblib import load, dump\n",
    "# from tqdm import tqdm           # console-based\n",
    "# from tqdm.notebook import tqdm  # jupyter-based\n",
    "from tqdm.auto import tqdm        # automatically selects\n",
    "# tqdm._instances.clear()\n",
    "\n",
    "# Import sklearn for machine learning\n",
    "import sklearn\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "# import mlflow\n",
    "# import wandb\n",
    "# import prefect\n",
    "# from prefect import task, flow, Flow\n",
    "# from prefect.tasks import task_input_hash\n",
    "# from prefect.artifacts import create_markdown_artifact\n",
    "\n",
    "# Import Evidently for data drift and model performance monitoring\n",
    "# from evidently import ColumnMapping\n",
    "# from evidently.report import Report\n",
    "# from evidently.metrics import ColumnDriftMetric, DatasetDriftMetric\n",
    "# from evidently.metrics import DatasetMissingValuesMetric, ColumnQuantileMetric\n",
    "\n",
    "# memory management performs garbage collection \n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb564b98",
   "metadata": {},
   "source": [
    "<div style=\"letter-spacing:normal; opacity:1.;\">\n",
    "  <h1 style=\"text-align:center; background-color: lightsalmon; color: Jaguar; border-radius:10px; font-family:monospace; border-radius:20px;\n",
    "            line-height:1.4; font-size:32px; font-weight:bold; text-transform: uppercase; padding: 9px;\">\n",
    "            <strong>2. Recognizing and Understanding Data</strong></h1>   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb5b120",
   "metadata": {},
   "source": [
    "## Ingest Data [wget](https://linuxways.net/centos/linux-wget-command-with-examples/) or [curl](https://daniel.haxx.se/blog/2020/09/10/store-the-curl-output-over-there/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e2a3fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Yellow Taxi Trip Records\" Download the data for January, February 2022\n",
    "# !wget -N -P \"./data\" https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet -q\n",
    "!wget -N -P \"./data\" https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-02.parquet -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "044ce591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/yellow_tripdata_2022-02.parquet']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob(f'./data/*.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23feabba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Get the current working directory\n",
    "# current_dir = os.getcwd()\n",
    "\n",
    "# Create a new directory for storing MLflow data\n",
    "os.makedirs('./pycode', exist_ok=True)\n",
    "os.makedirs('./tests', exist_ok=True)\n",
    "# os.makedirs('./data', exist_ok=True)\n",
    "# os.makedirs('./output', exist_ok=True)\n",
    "# os.makedirs('./models', exist_ok=True)\n",
    "# os.makedirs('./config', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d0d222",
   "metadata": {},
   "source": [
    "# Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38ea3d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "\n",
    "FROM python:3.10.0-slim\n",
    "\n",
    "RUN pip install -U pip & pip install pipenv\n",
    "\n",
    "COPY [ \"Pipfile\", \"Pipfile.lock\", \"./\" ]\n",
    "\n",
    "RUN pipenv install --system --deploy\n",
    "\n",
    "COPY [ \"pycode/batch.py\", \"pycode/batch.py\" ]\n",
    "COPY [ \"models/model.bin\", \"models/model.bin\" ]\n",
    "\n",
    "ENTRYPOINT [ \"python\", \"pycode/batch.py\" ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f41e352",
   "metadata": {},
   "source": [
    "# Baseline Model nyc_taxi Data\n",
    "\n",
    "## Q1. Refactoring\n",
    "\n",
    "Before we can start converting our code with tests, we need to \n",
    "refactor it. We'll start by getting rid of all the global variables. \n",
    "\n",
    "* Let's create a function `main` with two parameters: `year` and\n",
    "`month`.\n",
    "* Move all the code (except `read_data`) inside `main`\n",
    "* Make `categorical` a parameter for `read_data` and pass it inside `main`\n",
    "\n",
    "Now we need to create the \"main\" block from which we'll invoke\n",
    "the main function. How does the `if` statement that we use for\n",
    "this looks like? \n",
    "\n",
    "\n",
    "Hint: after refactoring, check that the code still works. Just run\n",
    "it e.g. for Feb 2022 and see if it finishes successfully. \n",
    "\n",
    "To make it easier to run it, you can write results to your local\n",
    "filesystem. E.g. here:\n",
    "\n",
    "```python\n",
    "output_file = f'taxi_type=yellow_year={year:04d}_month={month:02d}.parquet'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dcad358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./pycode/batch.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pycode/batch.py\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "tqdm._instances.clear()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def prepare_data(df, categorical) -> pd.DataFrame:\n",
    "    df[\"tpep_dropoff_datetime\"] = pd.to_datetime(df.tpep_dropoff_datetime)\n",
    "    df[\"tpep_pickup_datetime\"] = pd.to_datetime(df.tpep_pickup_datetime)\n",
    "\n",
    "    df[\"duration\"] = df[\"tpep_dropoff_datetime\"] - df[\"tpep_pickup_datetime\"]\n",
    "    df['duration'] = df['duration'].dt.total_seconds() / 60\n",
    "\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)].copy()\n",
    "\n",
    "    # df[categorical] = df[categorical].astype(str)\n",
    "    df[categorical] = df[categorical].fillna(-1).astype('int').astype('str')\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_data(filename, categorical) -> pd.DataFrame:\n",
    "    S3_ENDPOINT_URL = os.getenv('S3_ENDPOINT_URL')\n",
    "\n",
    "    \"\"\"Read data into DataFrame\"\"\"\n",
    "    if S3_ENDPOINT_URL is not None:\n",
    "        options = {\n",
    "            'client_kwargs': {\n",
    "                'endpoint_url': S3_ENDPOINT_URL\n",
    "            }\n",
    "        }\n",
    "\n",
    "        df = pd.read_parquet(filename, storage_options=options)\n",
    "    else:\n",
    "        df = pd.read_parquet(filename)\n",
    "\n",
    "    return prepare_data(df, categorical)\n",
    "\n",
    "\n",
    "def predict_duration(df: pd.DataFrame, categorical, dv, lr) -> np.ndarray:\n",
    "    \"\"\"Predict the duration using the trained model\"\"\"\n",
    "    dicts  = df[categorical].to_dict(orient='records')\n",
    "    X_val  = dv.transform(dicts)\n",
    "    y_pred = lr.predict(X_val)\n",
    "    return y_pred\n",
    "\n",
    "        \n",
    "def save_results(df: pd.DataFrame, y_pred: np.ndarray, output_file: str) -> None:\n",
    "    df_result = pd.DataFrame()\n",
    "    df_result['ride_id'] = df['ride_id']\n",
    "    df_result['predicted_duration'] = y_pred    \n",
    "    S3_ENDPOINT_URL = os.getenv('S3_ENDPOINT_URL')   \n",
    "\n",
    "    \"\"\"Save the predicted results to a parquet file\"\"\" \n",
    "    if S3_ENDPOINT_URL is not None:\n",
    "        options = {\n",
    "            'client_kwargs': {\n",
    "                'endpoint_url': S3_ENDPOINT_URL\n",
    "            }\n",
    "        }\n",
    "\n",
    "        df.to_parquet(output_file, engine='pyarrow', index=False, storage_options=options)\n",
    "    else:\n",
    "        os.makedirs('output', exist_ok=True)        \n",
    "        df_result.to_parquet(        \n",
    "            output_file,\n",
    "            engine='pyarrow',\n",
    "            # compression=None,\n",
    "            index=False,\n",
    "            storage_options=None\n",
    "        )\n",
    "    return None \n",
    "\n",
    "    \n",
    "def main(year, month):\n",
    "    categorical = ['PULocationID', 'DOLocationID']\n",
    "    # input_file  = f'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{year:04d}-{month:02d}.parquet'   \n",
    "    input_file  = f'./data/yellow_tripdata_{year:04d}-{month:02d}.parquet' \n",
    "    output_file = f'output/yellow_tripdata_{year:04d}-{month:02d}.parquet'\n",
    "  \n",
    "    steps = [\"Loading model\", \"Reading data\", \"Predict data\"]\n",
    "    with tqdm(total=len(steps), desc=\"Running steps\", leave=True) as pbar:\n",
    "        # Step 1: Loading model\n",
    "        pbar.set_description(steps[0])\n",
    "        with open('models/model.bin', 'rb') as f_in:\n",
    "            dv, lr = pickle.load(f_in)\n",
    "        pbar.update(1)\n",
    "\n",
    "        # Step 2: Reading data\n",
    "        pbar.set_description(steps[1])\n",
    "        df = read_data(input_file, categorical)\n",
    "        df['ride_id'] = f'{year:04d}/{month:02d}_' + df.index.astype('str')\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Step 3: Predict data\n",
    "        pbar.set_description(steps[2])\n",
    "        y_pred = predict_duration(df, categorical, dv, lr)\n",
    "        pbar.update(1)\n",
    "        pbar.close()    \n",
    "        \n",
    "\n",
    "    # Print Prediction\n",
    "    print('predicted mean duration:', y_pred.mean().round(2))\n",
    "    print('predicted sum duration:', y_pred.sum().round(2))\n",
    "\n",
    "    # save_results\n",
    "    save_results(df, y_pred, output_file)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':    \n",
    "    # Global Parameters\n",
    "    year        = int(sys.argv[1]) # 2022\n",
    "    month       = int(sys.argv[2]) # 2 \n",
    "    \n",
    "    main(year, month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e505195f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict data: 100%|███████████████████████████████| 3/3 [01:38<00:00, 32.96s/it]\n",
      "predicted mean duration: 12.51\n",
      "predicted sum duration: 36516505.75\n"
     ]
    }
   ],
   "source": [
    "!python ./pycode/batch.py 2022 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159982ef",
   "metadata": {},
   "source": [
    "## Q2. Installing pytest\n",
    "\n",
    "Now we need to install `pytest`:\n",
    "\n",
    "```bash\n",
    "pipenv install --dev pytest\n",
    "```\n",
    "\n",
    "Next, create a folder `tests` and then two files inside. \n",
    "\n",
    "The first one will be the file with tests. We can name it `test_batch.py`. \n",
    "\n",
    "The second file will be `__init__.py`. So, why do we need this second file?\n",
    "\n",
    "- To define a package and specify its boundaries\n",
    "- To manage the import of modules from the package \n",
    "- Both of the above options are correct\n",
    "- To initialize a new object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c70bbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./tests/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./tests/__init__.py\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f0ae20",
   "metadata": {},
   "source": [
    "In Python, the __init__.py file is a special file that is used to mark a directory as a Python package. When a directory contains an __init__.py file, Python recognizes it as a package and allows the directory to be treated as a module.\n",
    "\n",
    "Here are a few key points about the __init__.py file:\n",
    "\n",
    "1. Package Initialization: The primary purpose of the __init__.py file is to initialize the package when it is imported. It can contain Python code that sets up the package's environment, imports necessary modules, or performs other initialization tasks. This code is executed automatically when the package is imported.\n",
    "\n",
    "2. Namespace Definition: The __init__.py file also defines the package's namespace. It specifies which modules and sub-packages are part of the package and can be accessed when the package is imported. By importing specific modules in the __init__.py file, you can control the names available to users when they import the package.\n",
    "\n",
    "3. Python Package Structure: The presence of __init__.py files in directories helps define the structure of a Python package. It allows for hierarchical organization by enabling the creation of sub-packages within a package. Each sub-package can have its own __init__.py file, further defining its namespace and initialization logic.\n",
    "\n",
    "4. Compatibility: The use of __init__.py is a convention that has been established in Python to define packages. By adhering to this convention, developers ensure that their code is compatible with Python's import system and can be easily understood by other Python developers.\n",
    "\n",
    "Overall, the __init__.py file is crucial in Python packages as it enables proper package initialization, defines namespaces, and contributes to the organization and structure of a package. It helps ensure modularity, reusability, and maintainability in Python projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4284e92c",
   "metadata": {},
   "source": [
    "## Q3. Writing first unit test\n",
    "\n",
    "Now let's cover our code with unit tests.\n",
    "\n",
    "We'll start with the pre-processing logic inside `read_data`.\n",
    "\n",
    "It's difficult to test right now because first reads\n",
    "the file and then performs some transformations. We need to split this \n",
    "code into two parts: reading (I/O) and transformation. \n",
    "\n",
    "So let's create a function `prepare_data` that takes in a dataframe \n",
    "(and some other parameters too) and applies some transformation to it.\n",
    "\n",
    "(That's basically the entire `read_data` function after reading \n",
    "the parquet file)\n",
    "\n",
    "Now create a test and use this as input:\n",
    "\n",
    "```python\n",
    "data = [\n",
    "    (None, None, dt(1, 2), dt(1, 10)),\n",
    "    (1, None, dt(1, 2), dt(1, 10)),\n",
    "    (1, 2, dt(2, 2), dt(2, 3)),\n",
    "    (None, 1, dt(1, 2, 0), dt(1, 2, 50)),\n",
    "    (2, 3, dt(1, 2, 0), dt(1, 2, 59)),\n",
    "    (3, 4, dt(1, 2, 0), dt(2, 2, 1)),     \n",
    "]\n",
    "\n",
    "columns = ['PULocationID', 'DOLocationID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "```\n",
    "\n",
    "Where `dt` is a helper function:\n",
    "\n",
    "```python\n",
    "from datetime import datetime\n",
    "\n",
    "def dt(hour, minute, second=0):\n",
    "    return datetime(2022, 1, 1, hour, minute, second)\n",
    "```\n",
    "\n",
    "Define the expected output and use the assert to make sure \n",
    "that the actual dataframe matches the expected one\n",
    "\n",
    "Tip: When you compare two Pandas DataFrames, the result is also a DataFrame.\n",
    "The same is true for Pandas Series. Also, a DataFrame could be turned into a\n",
    "list of dictionaries.  \n",
    "\n",
    "How many rows should be there in the expected dataframe?\n",
    "\n",
    "- 1\n",
    "- 2\n",
    "- 3\n",
    "- 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0fafe28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./tests/test_batch.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./tests/test_batch.py\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Append the parent directory of the tests folder to the system path\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "\n",
    "from pycode import batch\n",
    "from datetime import datetime\n",
    "\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "tqdm._instances.clear()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Timestamp\n",
    "import pandas.testing as pd_testing\n",
    "\n",
    "\n",
    "def dt(hour, minute, second=0):\n",
    "    return datetime(2022, 1, 1, hour, minute, second)\n",
    "\n",
    "\n",
    "def test_read_data() -> pd.DataFrame:\n",
    "    \"\"\"Read data into DataFrame\"\"\"\n",
    "    data    = [\n",
    "        (None, None, dt(1, 2), dt(1, 10)),\n",
    "        (1, None, dt(1, 2), dt(1, 10)),\n",
    "        (1, 2, dt(2, 2), dt(2, 3)),\n",
    "        (None, 1, dt(1, 2, 0), dt(1, 2, 50)),\n",
    "        (2, 3, dt(1, 2, 0), dt(1, 2, 59)),\n",
    "        (3, 4, dt(1, 2, 0), dt(2, 2, 1)),     \n",
    "    ]\n",
    "    categorical = ['PULocationID', 'DOLocationID']   \n",
    "    columns = ['PULocationID', 'DOLocationID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime']\n",
    "\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    df[\"tpep_dropoff_datetime\"] = pd.to_datetime(df.tpep_dropoff_datetime)\n",
    "    df[\"tpep_pickup_datetime\"] = pd.to_datetime(df.tpep_pickup_datetime)\n",
    "\n",
    "    df[\"duration\"] = df[\"tpep_dropoff_datetime\"] - df[\"tpep_pickup_datetime\"]\n",
    "    df['duration'] = df['duration'].dt.total_seconds() / 60\n",
    "\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)].copy()\n",
    "\n",
    "    # df[categorical] = df[categorical].astype(str)\n",
    "    df[categorical] = df[categorical].fillna(-1).astype('int').astype('str')\n",
    "    print(df)\n",
    "\n",
    "    actual_features = df\n",
    "    # print(df.to_dict())\n",
    "    \n",
    "    expected_data = {\n",
    "        'PULocationID': {0: '-1', 1: '1', 2: '1'}, \n",
    "        'DOLocationID': {0: '-1', 1: '-1', 2: '2'}, \n",
    "        'tpep_pickup_datetime': {0: Timestamp('2022-01-01 01:02:00'), 1: Timestamp('2022-01-01 01:02:00'), 2: Timestamp('2022-01-01 02:02:00')}, \n",
    "        'tpep_dropoff_datetime': {0: Timestamp('2022-01-01 01:10:00'), 1: Timestamp('2022-01-01 01:10:00'), 2: Timestamp('2022-01-01 02:03:00')}, \n",
    "        'duration': {0: 8.0, 1: 8.0, 2: 1.0}\n",
    "    }\n",
    "    expected_features = pd.DataFrame(expected_data)\n",
    "\n",
    "    assert actual_features.equals(expected_features)\n",
    "    assert (actual_features == expected_features).all().all()\n",
    "    # pd_testing.assert_frame_equal(actual_features, expected_features)\n",
    "\n",
    "    return df\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    test_read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44ea88d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PULocationID DOLocationID  ... tpep_dropoff_datetime duration\n",
      "0           -1           -1  ...   2022-01-01 01:10:00      8.0\n",
      "1            1           -1  ...   2022-01-01 01:10:00      8.0\n",
      "2            1            2  ...   2022-01-01 02:03:00      1.0\n",
      "\n",
      "[3 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "!python ./tests/test_batch.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cebdce2",
   "metadata": {},
   "source": [
    "## Q4. Mocking S3 with Localstack \n",
    "\n",
    "Now let's prepare for an integration test. In our script, we \n",
    "write data to S3. So we'll use Localstack to mimic S3.\n",
    "\n",
    "First, let's run Localstack with Docker compose. Let's create a \n",
    "`docker-compose.yaml` file with just one service: localstack. Inside\n",
    "localstack, we're only interested in running S3. \n",
    "\n",
    "Start the service and test it by creating a bucket where we'll\n",
    "keep the output. Let's call it \"nyc-duration\".\n",
    "\n",
    "With AWS CLI, this is how we create a bucket:\n",
    "\n",
    "```bash\n",
    "aws s3 mb s3://nyc-duration\n",
    "```\n",
    "\n",
    "Then we need to check that the bucket was successfully created. With AWS, this is how we typically do it:\n",
    "\n",
    "```bash\n",
    "aws s3 ls\n",
    "```\n",
    "\n",
    "In both cases we should adjust commands for localstack. Which option do we need to use for such purposes?\n",
    "\n",
    "- `--endpoint-url`\n",
    "- `--profile`\n",
    "- `--region`\n",
    "- `--version`\n",
    "\n",
    "\n",
    "## Make input and output paths configurable\n",
    "\n",
    "Right now the input and output paths are hardcoded, but we want\n",
    "to change it for the tests. \n",
    "\n",
    "One of the possible ways would be to specify `INPUT_FILE_PATTERN` and `OUTPUT_FILE_PATTERN` via the env \n",
    "variables. Let's do that:\n",
    "\n",
    "\n",
    "```bash\n",
    "export INPUT_FILE_PATTERN=\"s3://nyc-duration/in/{year:04d}-{month:02d}.parquet\"\n",
    "export OUTPUT_FILE_PATTERN=\"s3://nyc-duration/out/{year:04d}-{month:02d}.parquet\"\n",
    "```\n",
    "\n",
    "And this is how we can read them:\n",
    "\n",
    "```python\n",
    "def get_input_path(year, month):\n",
    "    default_input_pattern = 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{year:04d}-{month:02d}.parquet'\n",
    "    input_pattern = os.getenv('INPUT_FILE_PATTERN', default_input_pattern)\n",
    "    return input_pattern.format(year=year, month=month)\n",
    "\n",
    "\n",
    "def get_output_path(year, month):\n",
    "    default_output_pattern = 's3://nyc-duration-prediction-alexey/taxi_type=fhv/year={year:04d}/month={month:02d}/predictions.parquet'\n",
    "    output_pattern = os.getenv('OUTPUT_FILE_PATTERN', default_output_pattern)\n",
    "    return output_pattern.format(year=year, month=month)\n",
    "\n",
    "\n",
    "def main(year, month):\n",
    "    input_file = get_input_path(year, month)\n",
    "    output_file = get_output_path(year, month)\n",
    "    # rest of the main function ... \n",
    "```\n",
    "\n",
    "\n",
    "## Reading from Localstack S3 with Pandas\n",
    "\n",
    "So far we've been reading parquet files from S3 with using\n",
    "pandas `read_parquet`. But this way we read it from the\n",
    "actual S3 service. Now we need to replace it with our localstack\n",
    "one.\n",
    "\n",
    "For that, we need to specify the endpoint url:\n",
    "\n",
    "```python\n",
    "options = {\n",
    "    'client_kwargs': {\n",
    "        'endpoint_url': S3_ENDPOINT_URL\n",
    "    }\n",
    "}\n",
    "\n",
    "df = pd.read_parquet('s3://bucket/file.parquet', storage_options=options)\n",
    "```\n",
    "\n",
    "Let's modify our `read_data` function:\n",
    "\n",
    "- check if `S3_ENDPOINT_URL` is set, and if it is, use it for reading\n",
    "- otherwise use the usual way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6fda0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pycode/batch.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pycode/batch.py\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "tqdm._instances.clear()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def prepare_data(df, categorical) -> pd.DataFrame:\n",
    "    df[\"tpep_dropoff_datetime\"] = pd.to_datetime(df.tpep_dropoff_datetime)\n",
    "    df[\"tpep_pickup_datetime\"] = pd.to_datetime(df.tpep_pickup_datetime)\n",
    "\n",
    "    df[\"duration\"] = df[\"tpep_dropoff_datetime\"] - df[\"tpep_pickup_datetime\"]\n",
    "    df['duration'] = df['duration'].dt.total_seconds() / 60\n",
    "\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)].copy()\n",
    "\n",
    "    # df[categorical] = df[categorical].astype(str)\n",
    "    df[categorical] = df[categorical].fillna(-1).astype('int').astype('str')\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_data(filename, categorical) -> pd.DataFrame:\n",
    "    S3_ENDPOINT_URL = os.getenv('S3_ENDPOINT_URL')\n",
    "\n",
    "    \"\"\"Read data into DataFrame\"\"\"\n",
    "    if S3_ENDPOINT_URL is not None:\n",
    "        options = {\n",
    "            'client_kwargs': {\n",
    "                'endpoint_url': S3_ENDPOINT_URL\n",
    "            }\n",
    "        }\n",
    "\n",
    "        df = pd.read_parquet(filename, storage_options=options)\n",
    "    else:\n",
    "        df = pd.read_parquet(filename)\n",
    "\n",
    "    return prepare_data(df, categorical)\n",
    "\n",
    "\n",
    "def predict_duration(df: pd.DataFrame, categorical, dv, lr) -> np.ndarray:\n",
    "    \"\"\"Predict the duration using the trained model\"\"\"\n",
    "    dicts  = df[categorical].to_dict(orient='records')\n",
    "    X_val  = dv.transform(dicts)\n",
    "    y_pred = lr.predict(X_val)\n",
    "    return y_pred\n",
    "\n",
    "        \n",
    "def save_results(df: pd.DataFrame, y_pred: np.ndarray, output_file: str) -> None:\n",
    "    df_result = pd.DataFrame()\n",
    "    df_result['ride_id'] = df['ride_id']\n",
    "    df_result['predicted_duration'] = y_pred    \n",
    "    S3_ENDPOINT_URL = os.getenv('S3_ENDPOINT_URL')   \n",
    "\n",
    "    \"\"\"Save the predicted results to a parquet file\"\"\" \n",
    "    if S3_ENDPOINT_URL is not None:\n",
    "        options = {\n",
    "            'client_kwargs': {\n",
    "                'endpoint_url': S3_ENDPOINT_URL\n",
    "            }\n",
    "        }\n",
    "\n",
    "        df.to_parquet(output_file, engine='pyarrow', index=False, storage_options=options)\n",
    "    else:\n",
    "        os.makedirs('output', exist_ok=True)        \n",
    "        df_result.to_parquet(        \n",
    "            output_file,\n",
    "            engine='pyarrow',\n",
    "            # compression=None,\n",
    "            index=False,\n",
    "            storage_options=None\n",
    "        )\n",
    "    return None \n",
    "\n",
    "\n",
    "def get_input_path(year, month):\n",
    "    # default_input_pattern = f'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{year:04d}-{month:02d}.parquet'\n",
    "    default_input_pattern  = f'./data/yellow_tripdata_{year:04d}-{month:02d}.parquet'\n",
    "\n",
    "    input_pattern = os.getenv('INPUT_FILE_PATTERN', default_input_pattern)\n",
    "    return input_pattern.format(year=year, month=month)\n",
    "\n",
    "\n",
    "def get_output_path(year, month):\n",
    "    # default_output_pattern = f's3://nyc-duration-prediction-alexey/taxi_type=fhv/year={year:04d}/month={month:02d}/predictions.parquet'\n",
    "    default_output_pattern = f'output/yellow_tripdata_{year:04d}-{month:02d}.parquet' \n",
    "\n",
    "    output_pattern = os.getenv('OUTPUT_FILE_PATTERN', default_output_pattern)\n",
    "    return output_pattern.format(year=year, month=month)\n",
    "\n",
    "    \n",
    "def main(year, month):\n",
    "    categorical = ['PULocationID', 'DOLocationID']\n",
    "    input_file = get_input_path(year, month)\n",
    "    output_file = get_output_path(year, month)\n",
    "  \n",
    "    steps = [\"Loading model\", \"Reading data\", \"Predict data\"]\n",
    "    with tqdm(total=len(steps), desc=\"Running steps\", leave=True) as pbar:\n",
    "        # Step 1: Loading model\n",
    "        pbar.set_description(steps[0])\n",
    "        with open('models/model.bin', 'rb') as f_in:\n",
    "            dv, lr = pickle.load(f_in)\n",
    "        pbar.update(1)\n",
    "\n",
    "        # Step 2: Reading data\n",
    "        pbar.set_description(steps[1])\n",
    "        df = read_data(input_file, categorical)\n",
    "        df['ride_id'] = f'{year:04d}/{month:02d}_' + df.index.astype('str')\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Step 3: Predict data\n",
    "        pbar.set_description(steps[2])\n",
    "        y_pred = predict_duration(df, categorical, dv, lr)\n",
    "        pbar.update(1)\n",
    "        pbar.close()    \n",
    "        \n",
    "\n",
    "    # Print Prediction\n",
    "    print('predicted mean duration:', y_pred.mean().round(2))\n",
    "    print('predicted sum duration:', y_pred.sum().round(2))\n",
    "\n",
    "    # save_results\n",
    "    save_results(df, y_pred, output_file)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':    \n",
    "    # Global Parameters\n",
    "    year        = int(sys.argv[1]) # 2022\n",
    "    month       = int(sys.argv[2]) # 2 \n",
    "    \n",
    "    main(year, month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "711d9aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing docker-compose.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile docker-compose.yml\n",
    "\n",
    "# Specifies the Docker Compose file version\n",
    "version: '3.7'\n",
    "\n",
    "# Defines the services that make up your app\n",
    "services:\n",
    "  # Defines a service name\n",
    "  s3:\n",
    "    # Specifies the Docker image to use for this service\n",
    "    image: localstack/localstack\n",
    "    # Maps ports between the host and the container\n",
    "    ports:\n",
    "      - \"4566:4566\"\n",
    "    # Sets environment variables for the service\n",
    "    environment:\n",
    "      - SERVICES=s3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f89cba8",
   "metadata": {},
   "source": [
    "## Q5. Creating test data\n",
    "\n",
    "Now let's create `integration_test.py`\n",
    "\n",
    "We'll use the dataframe we created in Q3 (the dataframe for the unit test)\n",
    "and save it to S3. You don't need to do anything else: just create a dataframe \n",
    "and save it.\n",
    "\n",
    "We will pretend that this is data for January 2022.\n",
    "\n",
    "Run the `integration_test.py` script. After that, use AWS CLI to verify that the \n",
    "file was created. \n",
    "\n",
    "Use this snipped for saving the file:\n",
    "\n",
    "```python\n",
    "df_input.to_parquet(\n",
    "    input_file,\n",
    "    engine='pyarrow',\n",
    "    compression=None,\n",
    "    index=False,\n",
    "    storage_options=options\n",
    ")\n",
    "```\n",
    "\n",
    "What's the size of the file?\n",
    "\n",
    "- 3667\n",
    "- 23667\n",
    "- 43667\n",
    "- 63667\n",
    "\n",
    "Note: it's important to use the code from the snippet for saving\n",
    "the file. Otherwise the size may be different depending on the OS,\n",
    "engine and compression. Even if you use this exact snippet, the size\n",
    "of your dataframe may still be a bit off. Just select the closest option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ebbbfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./tests/integration_test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./tests/integration_test.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Append the parent directory of the tests folder to the system path\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "\n",
    "from pycode import batch\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def dt(hour, minute, second=0):\n",
    "    return datetime(2022, 1, 1, hour, minute, second)\n",
    "\n",
    " \n",
    "def test_read_data() -> pd.DataFrame:\n",
    "    \"\"\"Read data into DataFrame\"\"\"\n",
    "    data    = [\n",
    "        (None, None, dt(1, 2), dt(1, 10)),\n",
    "        (1, None, dt(1, 2), dt(1, 10)),\n",
    "        (1, 2, dt(2, 2), dt(2, 3)),\n",
    "        (None, 1, dt(1, 2, 0), dt(1, 2, 50)),\n",
    "        (2, 3, dt(1, 2, 0), dt(1, 2, 59)),\n",
    "        (3, 4, dt(1, 2, 0), dt(2, 2, 1)),     \n",
    "    ]\n",
    "    categorical = ['PULocationID', 'DOLocationID']   \n",
    "    columns = ['PULocationID', 'DOLocationID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime']\n",
    "\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "S3_ENDPOINT_URL = os.getenv('S3_ENDPOINT_URL')\n",
    "if S3_ENDPOINT_URL is not None:\n",
    "    options = {\n",
    "        'client_kwargs': {\n",
    "            'endpoint_url': S3_ENDPOINT_URL\n",
    "        }\n",
    "    }\n",
    "else:\n",
    "    options = None\n",
    "    \n",
    "\n",
    "\"\"\"Read data into DataFrame\"\"\"\n",
    "df_input = test_read_data()\n",
    "input_file = batch.get_input_path(2022, 1)\n",
    "output_file = batch.get_output_path(2022, 1)\n",
    "\n",
    "print(df_input)\n",
    "\n",
    "df_input.to_parquet(\n",
    "    input_file,\n",
    "    engine='pyarrow',\n",
    "    compression=None,\n",
    "    index=False,\n",
    "    storage_options=options\n",
    ")\n",
    "\n",
    "# Actual Data\n",
    "os.system('python pycode/batch.py 2022 1')\n",
    "df_actual = pd.read_parquet(output_file, storage_options=options)\n",
    "\n",
    "assert abs(df_actual['predicted_duration'].sum().round(2) - 31.51) < 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d970b3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PULocationID  DOLocationID tpep_pickup_datetime tpep_dropoff_datetime\n",
      "0           NaN           NaN  2022-01-01 01:02:00   2022-01-01 01:10:00\n",
      "1           1.0           NaN  2022-01-01 01:02:00   2022-01-01 01:10:00\n",
      "2           1.0           2.0  2022-01-01 02:02:00   2022-01-01 02:03:00\n",
      "3           NaN           1.0  2022-01-01 01:02:00   2022-01-01 01:02:50\n",
      "4           2.0           3.0  2022-01-01 01:02:00   2022-01-01 01:02:59\n",
      "5           3.0           4.0  2022-01-01 01:02:00   2022-01-01 02:02:01\n",
      "Predict data: 100%|███████████████████████████████| 3/3 [00:00<00:00,  3.05it/s]\n",
      "predicted mean duration: 10.5\n",
      "predicted sum duration: 31.51\n"
     ]
    }
   ],
   "source": [
    "!python ./tests/integration_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dbe819e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1995\n",
      "4.0K\n",
      "1995 ./output/yellow_tripdata_2022-01.parquet\n"
     ]
    }
   ],
   "source": [
    "# So what's the size of file?\n",
    "!stat ./output/yellow_tripdata_2022-01.parquet  | grep Size:       | awk '{print $2}'\n",
    "!du -h ./output/yellow_tripdata_2022-01.parquet | awk '{print $1}'\n",
    "!wc -c ./output/yellow_tripdata_2022-01.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "12fc29d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./tests/integration_test.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./tests/integration_test.sh\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "docker-compose up -d\n",
    "\n",
    "sleep 5\n",
    "\n",
    "export INPUT_FILE_PATTERN=\"s3://nyc-duration/in/{year:04d}-{month:02d}.parquet\"\n",
    "export OUTPUT_FILE_PATTERN=\"s3://nyc-duration/out/{year:04d}-{month:02d}.parquet\"\n",
    "export S3_ENDPOINT_URL=\"http://localhost:4566\"\n",
    "\n",
    "\n",
    "aws --endpoint-url=\"${S3_ENDPOINT_URL}\" s3 mb s3://nyc-duration\n",
    "\n",
    "pipenv run python integration_test.py\n",
    "\n",
    "ERROR_CODE=$?\n",
    "\n",
    "if [ ${ERROR_CODE} != 0 ]; then\n",
    "    docker-compose logs\n",
    "    docker-compose down\n",
    "    exit ${ERROR_CODE}\n",
    "fi\n",
    "\n",
    "echo \"yay tests work!\"\n",
    "\n",
    "docker-compose down"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c8ab5d",
   "metadata": {},
   "source": [
    "## Q6. Finish the integration test\n",
    "\n",
    "We can read from our localstack s3, but we also need to write to it.\n",
    "\n",
    "Create a function `save_data` which works similarly to `read_data`,\n",
    "but we use it for saving a dataframe. \n",
    "\n",
    "Let's run the `batch.py` script for \"January 2022\" (the fake data\n",
    "we created in Q5). \n",
    "\n",
    "We can do that from our integration test in Python: we can use\n",
    "`os.system` for doing that (there are other options too). \n",
    "\n",
    "Now it saves the result to localstack.\n",
    "\n",
    "The only thing we need to do now is to read this data and \n",
    "verify the result is correct. \n",
    "\n",
    "What's the sum of predicted durations for the test dataframe?\n",
    "\n",
    "- 10.50\n",
    "- 31.51\n",
    "- 59.28\n",
    "- 81.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d70febb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict data: 100%|███████████████████████████████| 3/3 [00:00<00:00,  3.33it/s]\n",
      "predicted mean duration: 10.5\n",
      "predicted sum duration: 31.51\n"
     ]
    }
   ],
   "source": [
    "!python ./pycode/batch.py 2022 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b316ead1",
   "metadata": {},
   "source": [
    "## Running the test (ungraded)\n",
    "\n",
    "The rest is ready, but we need to write a shell script for doing \n",
    "that. \n",
    "\n",
    "Let's do that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9a5e87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab7775c3",
   "metadata": {},
   "source": [
    "## Submit the results\n",
    "\n",
    "* Submit your results here: https://forms.gle/vi7k972SKLmpwohG8\n",
    "* It's possible that your answers won't match exactly. If it's the case, select the closest one.\n",
    "* You can submit your answers multiple times. In this case, the last submission will be used for scoring.\n",
    "\n",
    "## Deadline\n",
    "\n",
    "The deadline for submitting is 16 July (Sunday) 23:00 CEST. After that, the form will be closed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf34682f",
   "metadata": {},
   "source": [
    "# End of The Project"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "314.8px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
